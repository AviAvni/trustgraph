#!/usr/bin/env python3

import _jsonnet as j
import logging
import os
from simple_term_menu import TerminalMenu
import json

logger = logging.getLogger("setup-menu")
logger.setLevel(logging.INFO)

class Generator:

    def __init__(self, base="templates/"):
        self.jsonnet_base = base

    def process_jsonnet(self, config):

        res = j.evaluate_snippet("config", config, import_callback=self.load)
        return json.loads(res)

    def load(self, dir, filename):

        logger.debug("Request jsonnet: %s %s", dir, filename)

        try:
            if dir:
                path = os.path.join(".", dir, filename)
            else:
                path = os.path.join(self.jsonnet_base, filename)

            logger.debug("Try: %s", path)
            with open(path, "rb") as f:
                logger.debug("Loaded: %s", path)
                return str(path), f.read()

        except:
            path = os.path.join(self.jsonnet_base, filename)
            logger.debug("Try: %s", path)
            with open(path, "rb") as f:
                logger.debug("Loaded: %s", path)
                return str(path), f.read()

class Chooser:

    def describe(self, option):
        for opt in self.options:
            if option == opt[2]: return opt[3]
        return "No documentation on {option}"

    def list(self):
        return [
            f"[{opt[0]}] {opt[2]}"
            for opt in self.options
        ]

    def choose(self):
        print()
        print(self.prompt)
        
        menu = TerminalMenu(
            self.list(),
            preview_command=self.describe,
            preview_size=0.5,
            preview_title="description",
        )

        ix = menu.show()

        selection = self.options[ix][1]

        print()
        print(f"Selected: {self.options[ix][2]}")

        return selection
    
class Llms(Chooser):

    def __init__(self):
        self.prompt = "Select an LLM..."

        self.options = [
            ("a", "azure", "Azure",
             """The large language model (LLM) tool in prompt flow enables you to take
advantage of widely used large language models like OpenAI, Azure OpenAI
Service, or any language model supported by the Azure AI model inference
API for natural language processing."""),
            ("b", "bedrock", "AWS Bedrock",
             """Amazon Bedrock is a fully managed service that offers a choice of
high-performing foundation models (FMs) from leading AI companies like AI21
Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through
a single API, along with a broad set of capabilities you need to build
generative AI applications with security, privacy, and responsible AI. Using
Amazon Bedrock, you can easily experiment with and evaluate top FMs for your
use case, privately customize them with your data using techniques such as
fine-tuning and Retrieval Augmented Generation (RAG), and build agents that
execute tasks using your enterprise systems and data sources. Since Amazon
Bedrock is serverless, you don't have to manage any infrastructure, and you
can securely integrate and deploy generative AI capabilities into your
applications using the AWS services you are already familiar with."""),
            ("c", "claude", "Anthropic Claude",
             """Claude is a family of highly performant and intelligent AI models built by
Anthropic. While Claude is powerful and extensible, it's also the most
trustworthy and reliable AI available."""),
            ("h", "cohere", "Cohere",
             """Cohere Command R is a family of highly scalable language models that balance
high performance with strong accuracy."""),
            ("o", "ollama", "Ollama",
             """Ollama is an open-source project that serves as a powerful and user-friendly
platform for running LLMs on your local machine. It acts as a bridge between
the complexities of LLM technology and the desire for an accessible and
customizable AI experience.

To use Ollama you will need to have an Ollama service running already, and
have it loaded with the LLM you wish to use."""),
            ("v", "vertexai", "VertexAI",
             """Vertex AI is a machine learning (ML) platform that lets you train and deploy
ML models and AI applications, and customize large language models (LLMs)
for use in your AI-powered applications. Vertex AI combines data
engineering, data science, and ML engineering workflows, enabling your teams
to collaborate using a common toolset and scale your applications using the
benefits of Google Cloud.""")
        ]

class Chunkers(Chooser):

    def __init__(self):

        self.prompt = "Choose a chunker..."

        self.options = [
            ("t", "chunker-token", "tiktoken token-aware chunker",
             """TikToken is a fast byte pair encoding (BPE) tokenizer developed by OpenAI
for use with its models. Designed to be lightweight and efficient, TikToken
excels in converting text into tokens, which are sequences of numbers that
models can interpret. It is especially useful for applications where token
usage must be limited, such as in translation services or text generation."""),
            ("r", "chunker-recursive", "Recursive character chunker",
             """This text splitter is a fast, simple chunker for generic text. It tries to
split at paragraph bounders until the chunks are small enough.  This has the
effect of trying to keep all paragraphs (and then sentences, and then words)
together as long as possible, as those would generically seem to be the
strongest semantically related pieces of text."""),
        ]

class VectorStores(Chooser):

    def __init__(self):

        self.prompt = "Choose a vector store..."
 
        self.options = [
            ("m", "milvus", "Milvus vector store",
             """Milvus is an open-source vector database built for GenAI
applications. Install with pip, perform high-speed searches, and scale to
tens of billions of vectors with minimal performance loss."""),
        ]

class TripleStores(Chooser):        

    def __init__(self):

        self.prompt = "Choose a triple store..."

        self.options = [
            ("c", "cassandra", "Apache Cassandra",
             """Cassandra is not a native graph DB, but is a very powerful
columnar store.  Trustgraph knows how to drive Cassandra as a graph
database."""),     
            ("n", "neo4j", "Neo4j community version",
             """Neo4j is a simple, popular graph database.  It has an in-built
graph query UI to help visualise the graph store."""),
        ]

def main():

    llm = Llms().choose()
    chunker = Chunkers().choose()
    vector = VectorStores().choose()
    triple = TripleStores().choose()

    config = f"""
local triple = import "components/{triple}.jsonnet";
local pulsar = import "components/pulsar.jsonnet";
local vector = import "components/{vector}.jsonnet";
local grafana = import "components/grafana.jsonnet";
local trustgraph = import "components/trustgraph.jsonnet";
local ai = import "components/{llm}.jsonnet";

local config = triple + pulsar + vector + grafana + trustgraph + ai;

std.manifestYamlDoc(config)
"""

    print(config)

    g = Generator()

    resp = g.process_jsonnet(config)

    print(resp)

if __name__ == "__main__":
    main()

